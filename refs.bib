
@inproceedings{pattersonCocoAttributesAttributes2016,
  title = {Coco Attributes: {{Attributes}} for People, Animals, and Objects},
  booktitle = {European {{Conference}} on {{Computer Vision}}},
  publisher = {{Springer}},
  author = {Patterson, Genevieve and Hays, James},
  year = {2016},
  pages = {85--100}
}

@inproceedings{linMicrosoftCocoCommon2014,
  title = {Microsoft Coco: {{Common}} Objects in Context},
  booktitle = {European Conference on Computer Vision},
  publisher = {{Springer}},
  author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  year = {2014},
  pages = {740--755}
}

@article{monfortMomentsTimeDataset2019,
  title = {Moments in Time Dataset: One Million Videos for Event Understanding},
  journal = {IEEE transactions on pattern analysis and machine intelligence},
  author = {Monfort, Mathew and Andonian, Alex and Zhou, Bolei and Ramakrishnan, Kandan and Bargal, Sarah Adel and Yan, Yan and Brown, Lisa and Fan, Quanfu and Gutfreund, Dan and Vondrick, Carl and others},
  year = {2019}
}

@incollection{galNatureDrawing2012,
  title = {Nature's {{Drawing}}},
  language = {en},
  booktitle = {Baroque {{Science}}},
  publisher = {{University of Chicago Press}},
  author = {Gal and {Chen-Morris}},
  year = {2012},
  keywords = {science,sts,technology,Galileo,history,Hooke,Huygens,Kepler,Newton},
  pages = {117-160},
  file = {/Users/audrey/Zotero/storage/D7XWFM9T/Gal_ChenMorris_BaroqueScience_Chapter4.pdf}
}

@incollection{zhengMARSVideoBenchmark2016,
  address = {{Cham}},
  title = {{{MARS}}: {{A Video Benchmark}} for {{Large}}-{{Scale Person Re}}-{{Identification}}},
  volume = {9910},
  isbn = {978-3-319-46465-7 978-3-319-46466-4},
  shorttitle = {{{MARS}}},
  abstract = {This paper considers person re-identification (re-id) in videos. We introduce a new video re-id dataset, named Motion Analysis and Re-identification Set (MARS), a video extension of the Market1501 dataset. To our knowledge, MARS is the largest video re-id dataset to date. Containing 1,261 IDs and around 20,000 tracklets, it provides rich visual information compared to image-based datasets. Meanwhile, MARS reaches a step closer to practice. The tracklets are automatically generated by the Deformable Part Model (DPM) as pedestrian detector and the GMMCP tracker. A number of false detection/tracking results are also included as distractors which would exist predominantly in practical video databases. Extensive evaluation of the state-of-the-art methods including the space-time descriptors and CNN is presented. We show that CNN in classification mode can be trained from scratch using the consecutive bounding boxes of each identity. The learned CNN embedding outperforms other competing methods considerably and has good generalization ability on other video re-id datasets upon fine-tuning.},
  language = {en},
  booktitle = {Computer {{Vision}} \textendash{} {{ECCV}} 2016},
  publisher = {{Springer International Publishing}},
  doi = {10.1007/978-3-319-46466-4_52},
  author = {Zheng, Liang and Bie, Zhi and Sun, Yifan and Wang, Jingdong and Su, Chi and Wang, Shengjin and Tian, Qi},
  editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
  year = {2016},
  keywords = {dataset,image,re-identification},
  pages = {868-884},
  file = {/Users/audrey/Zotero/storage/EEDYZ36X/Zheng et al. - 2016 - MARS A Video Benchmark for Large-Scale Person Re-.pdf}
}

@inproceedings{zhengScalablePersonReidentification2015,
  address = {{Santiago, Chile}},
  title = {Scalable {{Person Re}}-Identification: {{A Benchmark}}},
  isbn = {978-1-4673-8391-2},
  shorttitle = {Scalable {{Person Re}}-Identification},
  abstract = {This paper contributes a new high quality dataset for person re-identification, named ``Market-1501''. Generally, current datasets: 1) are limited in scale; 2) consist of hand-drawn bboxes, which are unavailable under realistic settings; 3) have only one ground truth and one query image for each identity (close environment). To tackle these problems, the proposed Market-1501 dataset is featured in three aspects. First, it contains over 32,000 annotated bboxes, plus a distractor set of over 500K images, making it the largest person re-id dataset to date. Second, images in Market-1501 dataset are produced using the Deformable Part Model (DPM) as pedestrian detector. Third, our dataset is collected in an open system, where each identity has multiple images under each camera.},
  language = {en},
  booktitle = {2015 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  publisher = {{IEEE}},
  doi = {10.1109/ICCV.2015.133},
  author = {Zheng, Liang and Shen, Liyue and Tian, Lu and Wang, Shengjin and Wang, Jingdong and Tian, Qi},
  month = dec,
  year = {2015},
  keywords = {re-identification},
  pages = {1116-1124},
  file = {/Users/audrey/Zotero/storage/9YICTW9Q/Zheng et al. - 2015 - Scalable Person Re-identification A Benchmark.pdf}
}

@inproceedings{caesarCOCOStuffThingStuff2018,
  title = {{{COCO}}-{{Stuff}}: {{Thing}} and Stuff Classes in Context},
  booktitle = {Computer Vision and Pattern Recognition ({{CVPR}}), 2018 {{IEEE}} Conference On},
  publisher = {{IEEE}},
  author = {Caesar, Holger and Uijlings, Jasper and Ferrari, Vittorio},
  year = {2018},
  keywords = {dataset,image}
}

@article{dengImageNetLargeScaleHierarchical2009,
  title = {{{ImageNet}}: {{A Large}}-{{Scale Hierarchical Image Database}}},
  abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called ``ImageNet'', a largescale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 5001000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
  language = {en},
  author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and {Fei-Fei}, Li},
  year = {2009},
  keywords = {dataset,image},
  pages = {8},
  file = {/Users/audrey/Zotero/storage/8RWYSCEK/Deng et al. - ImageNet A Large-Scale Hierarchical Image Databas.pdf}
}

@inproceedings{chenPersonReidentificationDeep2017,
  address = {{Venice}},
  title = {Person {{Re}}-Identification by {{Deep Learning Multi}}-Scale {{Representations}}},
  isbn = {978-1-5386-1034-3},
  abstract = {Existing person re-identification (re-id) methods depend mostly on single-scale appearance information. This not only ignores the potentially useful explicit information of other different scales, but also loses the chance of mining the implicit correlated complementary advantages across scales. In this work, we demonstrate the benefits of learning multi-scale person appearance features using Convolutional Neural Networks (CNN) by aiming to jointly learn discriminative scale-specific features and maximise multiscale feature fusion selections in image pyramid inputs. Specifically, we formulate a novel Deep Pyramid Feature Learning (DPFL) CNN architecture for multi-scale appearance feature fusion optimised simultaneously by concurrent per-scale re-id losses and interactive cross-scale consensus regularisation in a closed-loop design. Extensive comparative evaluations demonstrate the re-id advantages of the proposed DPFL model over a wide range of state-ofthe-art re-id methods on three benchmarks Market-1501, CUHK03, and DukeMTMC-reID.},
  language = {en},
  booktitle = {2017 {{IEEE International Conference}} on {{Computer Vision Workshops}} ({{ICCVW}})},
  publisher = {{IEEE}},
  doi = {10.1109/ICCVW.2017.304},
  author = {Chen, Yanbei and Zhu, Xiatian and Gong, Shaogang},
  month = oct,
  year = {2017},
  keywords = {re-identification},
  pages = {2590-2600},
  file = {/Users/audrey/Zotero/storage/DLQWZF5L/Chen et al. - 2017 - Person Re-identification by Deep Learning Multi-sc.pdf}
}

@inproceedings{chengPersonReidentificationMultiChannel2016,
  address = {{Las Vegas, NV, USA}},
  title = {Person {{Re}}-Identification by {{Multi}}-{{Channel Parts}}-{{Based CNN}} with {{Improved Triplet Loss Function}}},
  isbn = {978-1-4673-8851-1},
  abstract = {Person re-identification across cameras remains a very challenging problem, especially when there are no overlapping fields of view between cameras. In this paper, we present a novel multi-channel parts-based convolutional neural network (CNN) model under the triplet framework for person re-identification. Specifically, the proposed CNN model consists of multiple channels to jointly learn both the global full-body and local body-parts features of the input persons. The CNN model is trained by an improved triplet loss function that serves to pull the instances of the same person closer, and at the same time push the instances belonging to different persons farther from each other in the learned feature space. Extensive comparative evaluations demonstrate that our proposed method significantly outperforms many state-of-the-art approaches, including both traditional and deep network-based ones, on the challenging i-LIDS, VIPeR, PRID2011 and CUHK01 datasets.},
  language = {en},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  publisher = {{IEEE}},
  doi = {10.1109/CVPR.2016.149},
  author = {Cheng, De and Gong, Yihong and Zhou, Sanping and Wang, Jinjun and Zheng, Nanning},
  month = jun,
  year = {2016},
  keywords = {re-identification,embedding,triplet loss},
  pages = {1335-1344},
  file = {/Users/audrey/Zotero/storage/44ZMX9IK/Cheng et al. - 2016 - Person Re-identification by Multi-Channel Parts-Ba.pdf}
}

@inproceedings{bolukbasiManComputerProgrammer2016,
  title = {Man Is to Computer Programmer as Woman Is to Homemaker? Debiasing Word Embeddings},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Bolukbasi, Tolga and Chang, Kai-Wei and Zou, James Y and Saligrama, Venkatesh and Kalai, Adam T},
  year = {2016},
  keywords = {word,embedding,bias},
  pages = {4349--4357}
}

@inproceedings{liuMultiScaleTripletCNN2016,
  address = {{Amsterdam, The Netherlands}},
  title = {Multi-{{Scale Triplet CNN}} for {{Person Re}}-{{Identification}}},
  isbn = {978-1-4503-3603-1},
  abstract = {Person re-identification aims at identifying a certain person across non-overlapping multi-camera networks. It is a fundamental and challenging task in automated video surveillance. Most existing researches mainly rely on hand-crafted features, resulting in unsatisfactory performance. In this paper, we propose a multi-scale triplet convolutional neural network which captures visual appearance of a person at various scales. We propose to optimize the network parameters by a comparative similarity loss on massive sample triplets, addressing the problem of small training set in person re-identification. In particular, we design a unified multi-scale network architecture consisting of both deep and shallow neural networks, towards learning robust and effective features for person re-identification under complex conditions. Extensive evaluation on the real-world Market-1501 dataset have demonstrated the effectiveness of the proposed approach.},
  language = {en},
  booktitle = {Proceedings of the 2016 {{ACM}} on {{Multimedia Conference}} - {{MM}} '16},
  publisher = {{ACM Press}},
  doi = {10.1145/2964284.2967209},
  author = {Liu, Jiawei and Zha, Zheng-Jun and Tian, Qi and Liu, Dong and Yao, Ting and Ling, Qiang and Mei, Tao},
  year = {2016},
  pages = {192-196},
  file = {/Users/audrey/Zotero/storage/U7SR4JZS/Liu et al. - 2016 - Multi-Scale Triplet CNN for Person Re-Identificati.pdf}
}

@article{hermansDefenseTripletLoss2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1703.07737},
  primaryClass = {cs},
  title = {In {{Defense}} of the {{Triplet Loss}} for {{Person Re}}-{{Identification}}},
  abstract = {In the past few years, the field of computer vision has gone through a revolution fueled mainly by the advent of large datasets and the adoption of deep convolutional neural networks for end-to-end learning. The person reidentification subfield is no exception to this. Unfortunately, a prevailing belief in the community seems to be that the triplet loss is inferior to using surrogate losses (classification, verification) followed by a separate metric learning step. We show that, for models trained from scratch as well as pretrained ones, using a variant of the triplet loss to perform end-to-end deep metric learning outperforms most other published methods by a large margin.},
  language = {en},
  journal = {arXiv:1703.07737 [cs]},
  author = {Hermans, Alexander and Beyer, Lucas and Leibe, Bastian},
  month = mar,
  year = {2017},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/audrey/Zotero/storage/B6QHVBGS/Hermans et al. - 2017 - In Defense of the Triplet Loss for Person Re-Ident.pdf}
}

@article{krizhevskyImageNetClassificationDeep2017,
  title = {{{ImageNet}} Classification with Deep Convolutional Neural Networks},
  volume = {60},
  issn = {00010782},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called ``dropout'' that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
  language = {en},
  number = {6},
  journal = {Communications of the ACM},
  doi = {10.1145/3065386},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  month = may,
  year = {2017},
  pages = {84-90},
  file = {/Users/audrey/Zotero/storage/G38JSHL4/Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional ne.pdf;/Users/audrey/Zotero/storage/XP9R34MI/Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional ne.pdf},
  ids = {krizhevskyImageNetClassificationDeep2017a}
}

@inproceedings{liDeepReIDDeepFilter2014,
  address = {{Columbus, OH, USA}},
  title = {{{DeepReID}}: {{Deep Filter Pairing Neural Network}} for {{Person Re}}-Identification},
  isbn = {978-1-4799-5118-5},
  shorttitle = {{{DeepReID}}},
  abstract = {Person re-identification is to match pedestrian images from disjoint camera views detected by pedestrian detectors. Challenges are presented in the form of complex variations of lightings, poses, viewpoints, blurring effects, image resolutions, camera settings, occlusions and background clutter across camera views. In addition, misalignment introduced by the pedestrian detector will affect most existing person re-identification methods that use manually cropped pedestrian images and assume perfect detection.},
  language = {en},
  booktitle = {2014 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  publisher = {{IEEE}},
  doi = {10.1109/CVPR.2014.27},
  author = {Li, Wei and Zhao, Rui and Xiao, Tong and Wang, Xiaogang},
  month = jun,
  year = {2014},
  pages = {152-159},
  file = {/Users/audrey/Zotero/storage/SLW6XY3J/Li et al. - 2014 - DeepReID Deep Filter Pairing Neural Network for P.pdf}
}

@article{johnsonPerceptualLossesRealTime2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1603.08155},
  primaryClass = {cs},
  title = {Perceptual {{Losses}} for {{Real}}-{{Time Style Transfer}} and {{Super}}-{{Resolution}}},
  abstract = {We consider image transformation problems, where an input image is transformed into an output image. Recent methods for such problems typically train feed-forward convolutional neural networks using a per-pixel loss between the output and ground-truth images. Parallel work has shown that high-quality images can be generated by defining and optimizing perceptual loss functions based on high-level features extracted from pretrained networks. We combine the benefits of both approaches, and propose the use of perceptual loss functions for training feed-forward networks for image transformation tasks. We show results on image style transfer, where a feed-forward network is trained to solve the optimization problem proposed by Gatys et al in real-time. Compared to the optimization-based method, our network gives similar qualitative results but is three orders of magnitude faster. We also experiment with single-image super-resolution, where replacing a per-pixel loss with a perceptual loss gives visually pleasing results.},
  language = {en},
  journal = {arXiv:1603.08155 [cs]},
  author = {Johnson, Justin and Alahi, Alexandre and {Fei-Fei}, Li},
  month = mar,
  year = {2016},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/audrey/Documents/Zotfiles/Johnson et al_2016_Perceptual Losses for Real-Time Style Transfer and Super-Resolution.pdf}
}

@inproceedings{carreiraQuoVadisAction2017,
  address = {{Honolulu, HI}},
  title = {Quo {{Vadis}}, {{Action Recognition}}? {{A New Model}} and the {{Kinetics Dataset}}},
  isbn = {978-1-5386-0457-1},
  shorttitle = {Quo {{Vadis}}, {{Action Recognition}}?},
  abstract = {The paucity of videos in current action classification datasets (UCF-101 and HMDB-51) has made it difficult to identify good video architectures, as most methods obtain similar performance on existing small-scale benchmarks. This paper re-evaluates state-of-the-art architectures in light of the new Kinetics Human Action Video dataset. Kinetics has two orders of magnitude more data, with 400 human action classes and over 400 clips per class, and is collected from realistic, challenging YouTube videos. We provide an analysis on how current architectures fare on the task of action classification on this dataset and how much performance improves on the smaller benchmark datasets after pre-training on Kinetics.

We also introduce a new two-stream inflated 3D convnet (I3D) that is based on 2D convnet inflation: filters and pooling kernels fo very deep image classification convnets are expanded into 3D, making it possible to learn seamless spatio-temporal feature extractors from video while leveraging successful ImageNet architecturwe designs and even their parameters. We show that, after pre-training on Kinetics, I3D models considerably improve upon the state-of-the-art in action classification, reaching 80.2\% on HMDB-51 and 97.9\% on UCF-101},
  language = {en},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  publisher = {{IEEE}},
  doi = {10.1109/CVPR.2017.502},
  author = {Carreira, Joao and Zisserman, Andrew},
  month = jul,
  year = {2017},
  pages = {4724-4733},
  file = {/Users/audrey/Zotero/storage/4Q855XFT/Carreira and Zisserman - 2017 - Quo Vadis, Action Recognition A New Model and the.pdf}
}

@article{hicksProgrammedInequality2017,
  title = {Programmed {{Inequality}}},
  language = {en},
  journal = {The MIT Press},
  author = {Hicks, Marie},
  year = {2017},
  pages = {354},
  file = {/Users/audrey/Zotero/storage/ZB9UX6FZ/Hicks - Programmed Inequality.pdf}
}

@inproceedings{rodriguezDensityawarePersonDetection2011,
  address = {{Barcelona, Spain}},
  title = {Density-Aware Person Detection and Tracking in Crowds},
  isbn = {978-1-4577-1102-2 978-1-4577-1101-5 978-1-4577-1100-8},
  abstract = {We address the problem of person detection and tracking in crowded video scenes. While the detection of individual objects has been improved significantly over the recent years, crowd scenes remain particularly challenging for the detection and tracking tasks due to heavy occlusions, high person densities and significant variation in people's appearance. To address these challenges, we propose to leverage information on the global structure of the scene and to resolve all detections jointly. In particular, we explore constraints imposed by the crowd density and formulate person detection as the optimization of a joint energy function combining crowd density estimation and the localization of individual people. We demonstrate how the optimization of such an energy function significantly improves person detection and tracking in crowds. We validate our approach on a challenging video dataset of crowded scenes.},
  language = {en},
  booktitle = {2011 {{International Conference}} on {{Computer Vision}}},
  publisher = {{IEEE}},
  doi = {10.1109/ICCV.2011.6126526},
  author = {Rodriguez, Mikel and Laptev, Ivan and Sivic, Josef and Audibert, Jean-Yves},
  month = nov,
  year = {2011},
  pages = {2423-2430},
  file = {/Users/audrey/Zotero/storage/DNEMIH34/Rodriguez et al. - 2011 - Density-aware person detection and tracking in cro.pdf}
}

@misc{strzalkowskiSocioBehavioralDimensionsIntelligence2019,
  title = {Socio-{{Behavioral Dimensions}} of {{Intelligence}}: {{Understanding}}, {{Manipulation}}, {{Defense}}.},
  author = {Strzalkowski, Tomek},
  year = {2019},
  file = {/Users/audrey/Zotero/storage/E46K9CC7/SoBeInt-rpi_1019_V2.pptx}
}

@misc{karpathyRecipeTrainingNeural2019,
  title = {A {{Recipe}} for {{Training Neural Networks}}},
  journal = {Andrej Karpathy Blog},
  howpublished = {http://karpathy.github.io/2019/04/25/recipe/},
  author = {Karpathy, Andrej},
  month = apr,
  year = {2019},
  file = {/Users/audrey/Zotero/storage/ZSM75V92/recipe.html}
}

@article{chunHabitsLeakingSluts2015,
  title = {Habits of {{Leaking}}: {{Of Sluts}} and {{Network Cards}}},
  volume = {26},
  issn = {1040-7391, 1527-1986},
  shorttitle = {Habits of {{Leaking}}},
  language = {en},
  number = {2},
  journal = {differences: A Journal of Feminist Cultural Studies},
  doi = {10.1215/10407391-3145937},
  author = {Chun, Wendy Hui Kyong and Friedland, Sarah},
  month = sep,
  year = {2015},
  pages = {1-28},
  file = {/Users/audrey/Zotero/storage/85GL3TJ8/Chun and Friedland - 2015 - Habits of Leaking Of Sluts and Network Cards.pdf}
}

@article{ben-aaronWeizenbaumExaminesComputers1985,
  address = {{Cambridge, Massachusetts}},
  title = {Weizenbaum Examines Computers and Society},
  language = {en},
  journal = {The Tech},
  author = {{ben-Aaron}, Diana},
  month = apr,
  year = {1985},
  pages = {16},
  file = {/Users/audrey/Zotero/storage/Q7KMPYJ2/Whang et al. - Financial aid cuts reduced.pdf}
}

@article{rosenblattPerceptronProbabilisticModel1958,
  title = {The Perceptron: {{A}} Probabilistic Model for Information Storage and Organization in the Brain.},
  volume = {65},
  issn = {1939-1471, 0033-295X},
  shorttitle = {The Perceptron},
  language = {en},
  number = {6},
  journal = {Psychological Review},
  doi = {10.1037/h0042519},
  author = {Rosenblatt, F.},
  year = {1958},
  pages = {386-408},
  file = {/Users/audrey/Zotero/storage/2NMYZ85A/Rosenblatt - 1958 - The perceptron A probabilistic model for informat.pdf}
}

@book{hayekSensoryOrder1952,
  address = {{Chicago, IL, USA}},
  title = {The {{Sensory Order}}},
  language = {English},
  publisher = {{University of Chicago Press}},
  author = {Hayek, F.A.},
  year = {1952},
  keywords = {_tablet},
  file = {/Users/audrey/Zotero/storage/V9PNCLKT/Hayek_1952_The Sensory Order.pdf}
}

@book{steeleEconomicsFriedrichHayek2007,
  address = {{Basingstoke [England] ; New York}},
  edition = {2nd ed},
  title = {The {{Economics}} of {{Friedrich Hayek}}},
  isbn = {978-1-4039-4352-1},
  lccn = {HB101.H39 S73 2007},
  language = {en},
  publisher = {{Palgrave Macmillan}},
  author = {Steele, G. R.},
  year = {2007},
  keywords = {20th century,History,Austrian school of economics,Economics,Friedrich August,Hayek; Friedrich A. von},
  file = {/Users/audrey/Zotero/storage/WLGNPB2A/Steele - 2007 - The economics of Friedrich Hayek.pdf},
  ids = {steeleEconomicsFriedrichHayek2007a,steeleEconomicsFriedrichHayek2007b},
  note = {OCLC: 70335335}
}

@article{birnerMindMarketSociety,
  title = {Mind, {{Market}} and {{Society Network Structures}} in the {{Work}} of {{F}}. {{A}}. {{Hayek}}},
  language = {en},
  author = {Birner, Jack},
  pages = {26},
  file = {/Users/audrey/Zotero/storage/I2XYFADV/Birner - Mind, Market and Society Network Structures in the.pdf},
  ids = {birnerMindMarketSocietya,birnerMindMarketSocietyb}
}

@article{westHowGoogleInterferes2019,
  chapter = {Tech},
  title = {How {{Google Interferes With Its Search Algorithms}} and {{Changes Your Results}}},
  issn = {0099-9660},
  abstract = {Pressed by businesses, interest groups and governments, the internet giant uses blacklists, algorithm tweaks and an army of contractors to shape what you see.},
  language = {en-US},
  journal = {Wall Street Journal},
  author = {West, Sam Schechner, Robert McMillan {and} John, Kirsten Grind},
  month = nov,
  year = {2019},
  keywords = {technology,abortion,advertising,armed forces,asset management,corporate,earnings,economic news,financial investments,financial performance,financial services,fund,general news,Google,health,industrial news,internet search engines,investing,Larry Page,leder,lifestyle,living,marketing,medical treatments,national,online service providers,personal technology,political,politics \& policy,portfolio,procedures,public security,securities,Sergey Brin,Sundar Pichai,wsjexchange},
  file = {/Users/audrey/Zotero/storage/EXF8AWYH/how-google-interferes-with-its-search-algorithms-and-changes-your-results-11573823753.html}
}

@incollection{collierMythAnalogAfrica,
  title = {The {{Myth}} of {{Analog Africa}}},
  booktitle = {Repainting the {{Walls}} of {{Lunda}}},
  author = {Collier, Delinda},
  keywords = {_tablet},
  file = {/Users/audrey/Zotero/storage/PMF4ETK8/Collier_The Myth of Analog Africa.pdf}
}

@article{narayananHowRecognizeAI,
  title = {How to Recognize {{AI}} Snake Oil},
  language = {en},
  author = {Narayanan, Arvind},
  pages = {21},
  file = {/Users/audrey/Zotero/storage/V5SSLAS2/Narayanan - How to recognize AI snake oil.pdf}
}

@inproceedings{caiHumanCenteredToolsCoping2019,
  address = {{Glasgow, Scotland Uk}},
  title = {Human-{{Centered Tools}} for {{Coping}} with {{Imperfect Algorithms During Medical Decision}}-{{Making}}},
  isbn = {978-1-4503-5970-2},
  abstract = {Machine learning (ML) is increasingly being used in image retrieval systems for medical decision making. One application of ML is to retrieve visually similar medical images from past patients (e.g. tissue from biopsies) to reference when making a medical decision with a new patient. However, no algorithm can perfectly capture an expert's ideal notion of similarity for every case: an image that is algorithmically determined to be similar may not be medically relevant to a doctor's specific diagnostic needs. In this paper, we identified the needs of pathologists when searching for similar images retrieved using a deep learning algorithm, and developed tools that empower users to cope with the search algorithm on-the-fly, communicating what types of similarity are most important at different moments in time. In two evaluations with pathologists, we found that these refinement tools increased the diagnostic utility of images found and increased user trust in the algorithm. The tools were preferred over a traditional interface, without a loss in diagnostic accuracy. We also observed that users adopted new strategies when using refinement tools, re-purposing them to test and understand the underlying algorithm and to disambiguate ML errors from their own errors. Taken together, these findings inform future human-ML collaborative systems for expert decision-making.},
  language = {en},
  booktitle = {Proceedings of the 2019 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}  - {{CHI}} '19},
  publisher = {{ACM Press}},
  doi = {10.1145/3290605.3300234},
  author = {Cai, Carrie J. and Stumpe, Martin C. and Terry, Michael and Reif, Emily and Hegde, Narayan and Hipp, Jason and Kim, Been and Smilkov, Daniel and Wattenberg, Martin and Viegas, Fernanda and Corrado, Greg S.},
  year = {2019},
  pages = {1-14},
  file = {/Users/audrey/Zotero/storage/NHH7V27S/Cai et al. - 2019 - Human-Centered Tools for Coping with Imperfect Alg.pdf}
}

@inproceedings{dragicevicIncreasingTransparencyResearch2019,
  address = {{Glasgow, Scotland Uk}},
  title = {Increasing the {{Transparency}} of {{Research Papers}} with {{Explorable Multiverse Analyses}}},
  isbn = {978-1-4503-5970-2},
  abstract = {We present explorable multiverse analysis reports, a new approach to statistical reporting where readers of research papers can explore alternative analysis options by interacting with the paper itself. This approach draws from two recent ideas: i) multiverse analysis, a philosophy of statistical reporting where paper authors report the outcomes of many different statistical analyses in order to show how fragile or robust their findings are; and ii) explorable explanations, narratives that can be read as normal explanations but where the reader can also become active by dynamically changing some elements of the explanation. Based on five examples and a design space analysis, we show how combining those two ideas can complement existing reporting approaches and constitute a step towards more transparent research papers.},
  language = {en},
  booktitle = {Proceedings of the 2019 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}  - {{CHI}} '19},
  publisher = {{ACM Press}},
  doi = {10.1145/3290605.3300295},
  author = {Dragicevic, Pierre and Jansen, Yvonne and Sarma, Abhraneel and Kay, Matthew and Chevalier, Fanny},
  year = {2019},
  pages = {1-15},
  file = {/Users/audrey/Zotero/storage/9J8HC3Z9/Dragicevic et al. - 2019 - Increasing the Transparency of Research Papers wit.pdf}
}

@article{gonenLipstickPigDebiasing2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1903.03862},
  primaryClass = {cs},
  title = {Lipstick on a {{Pig}}: {{Debiasing Methods Cover}} up {{Systematic Gender Biases}} in {{Word Embeddings But}} Do Not {{Remove Them}}},
  shorttitle = {Lipstick on a {{Pig}}},
  abstract = {Word embeddings are widely used in NLP for a vast range of tasks. It was shown that word embeddings derived from text corpora reflect gender biases in society. This phenomenon is pervasive and consistent across different word embedding models, causing serious concern. Several recent works tackle this problem, and propose methods for significantly reducing this gender bias in word embeddings, demonstrating convincing results. However, we argue that this removal is superficial. While the bias is indeed substantially reduced according to the provided bias definition, the actual effect is mostly hiding the bias, not removing it. The gender bias information is still reflected in the distances between ``gender-neutralized'' words in the debiased embeddings, and can be recovered from them. We present a series of experiments to support this claim, for two debiasing methods. We conclude that existing bias removal techniques are insufficient, and should not be trusted for providing gender-neutral modeling.},
  language = {en},
  journal = {arXiv:1903.03862 [cs]},
  author = {Gonen, Hila and Goldberg, Yoav},
  month = sep,
  year = {2019},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/audrey/Zotero/storage/B8DXLSYQ/Gonen and Goldberg - 2019 - Lipstick on a Pig Debiasing Methods Cover up Syst.pdf}
}

@article{selvarajuGradCAMVisualExplanations,
  title = {Grad-{{CAM}}: {{Visual Explanations From Deep Networks}} via {{Gradient}}-{{Based Localization}}},
  abstract = {We propose a technique for producing `visual explanations' for decisions from a large class of Convolutional Neural Network (CNN)-based models, making them more transparent. Our approach \textendash{} Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept (say logits for `dog' or even a caption), flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept. Unlike previous approaches, GradCAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers (e.g. VGG), (2) CNNs used for structured outputs (e.g. captioning), (3) CNNs used in tasks with multi-modal inputs (e.g. visual question answering) or reinforcement learning, without architectural changes or re-training. We combine Grad-CAM with existing fine-grained visualizations to create a high-resolution class-discriminative visualization, Guided Grad-CAM, and apply it to image classification, image captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into failure modes of these models (showing that seemingly unreasonable predictions have reasonable explanations), (b) outperform previous methods on the ILSVRC-15 weakly-supervised localization task, (c) are more faithful to the underlying model, and (d) help achieve model generalization by identifying dataset bias. For image captioning and VQA, our visualizations show even non-attention based models can localize inputs. Finally, we design and conduct human studies to measure if Grad-CAM explanations help users establish appropriate trust in predictions from deep networks and show that Grad-CAM helps untrained users successfully discern a `stronger' deep network from a `weaker' one even when both make identical predictions. Our code is available at https: //github.com/ramprs/grad-cam/ along with a demo on CloudCV [2]1 and video at youtu.be/COjUB9Izk6E.},
  language = {en},
  author = {Selvaraju, Ramprasaath R and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  pages = {9},
  file = {/Users/audrey/Zotero/storage/D6AENDB2/Selvaraju et al. - Grad-CAM Visual Explanations From Deep Networks v.pdf}
}

@article{guidottiSurveyMethodsExplaining2018,
  title = {A {{Survey}} of {{Methods}} for {{Explaining Black Box Models}}},
  volume = {51},
  issn = {03600300},
  language = {en},
  number = {5},
  journal = {ACM Computing Surveys},
  doi = {10.1145/3236009},
  author = {Guidotti, Riccardo and Monreale, Anna and Ruggieri, Salvatore and Turini, Franco and Giannotti, Fosca and Pedreschi, Dino},
  month = aug,
  year = {2018},
  pages = {1-42},
  file = {/Users/audrey/Zotero/storage/KNEIT7BY/Guidotti et al. - 2018 - A Survey of Methods for Explaining Black Box Model.pdf}
}

@article{goyalMakingVQAMatter,
  title = {Making the v in {{VQA Matter}}: {{Elevating}} the {{Role}} of {{Image Understanding}} in {{Visual Question Answering}}},
  abstract = {Problems at the intersection of vision and language are of significant importance both as challenging research questions and for the rich set of applications they enable. However, inherent structure in our world and bias in our language tend to be a simpler signal for learning than visual modalities, resulting in models that ignore visual information, leading to an inflated sense of their capability.},
  language = {en},
  author = {Goyal, Yash and Khot, Tejas and {Summers-Stay}, Douglas and Batra, Dhruv and Parikh, Devi},
  pages = {10},
  file = {/Users/audrey/Zotero/storage/9J334G4K/Goyal et al. - Making the v in VQA Matter Elevating the Role of .pdf}
}

@article{caliskanSemanticsDerivedAutomatically2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1608.07187},
  title = {Semantics Derived Automatically from Language Corpora Contain Human-like Biases},
  volume = {356},
  issn = {0036-8075, 1095-9203},
  abstract = {Artificial intelligence and machine learning are in a period of astounding growth. However, there are concerns that these technologies may be used, either with or without intention, to perpetuate the prejudice and unfairness that unfortunately characterizes many human institutions. Here we show for the first time that human-like semantic biases result from the application of standard machine learning to ordinary language\textemdash{}the same sort of language humans are exposed to every day. We replicate a spectrum of standard human biases as exposed by the Implicit Association Test and other well-known psychological studies. We replicate these using a widely used, purely statistical machine-learning model\textemdash{}namely, the GloVe word embedding\textemdash{}trained on a corpus of text from the Web. Our results indicate that language itself contains recoverable and accurate imprints of our historic biases, whether these are morally neutral as towards insects or flowers, problematic as towards race or gender, or even simply veridical, reflecting the status quo for the distribution of gender with respect to careers or first names. These regularities are captured by machine learning along with the rest of semantics. In addition to our empirical findings concerning language, we also contribute new methods for evaluating bias in text, the Word Embedding Association Test (WEAT) and the Word Embedding Factual Association Test (WEFAT). Our results have implications not only for AI and machine learning, but also for the fields of psychology, sociology, and human ethics, since they raise the possibility that mere exposure to everyday language can account for the biases we replicate here.},
  language = {en},
  number = {6334},
  journal = {Science},
  doi = {10.1126/science.aal4230},
  author = {Caliskan, Aylin and Bryson, Joanna J. and Narayanan, Arvind},
  month = apr,
  year = {2017},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Computation and Language,Computer Science - Computers and Society},
  pages = {183-186},
  file = {/Users/audrey/Zotero/storage/5FZIHCC6/Caliskan et al. - 2017 - Semantics derived automatically from language corp.pdf}
}

@incollection{marshHayekCognitiveScientist2010,
  title = {Hayek: {{Cognitive}} Scientist {{Avant}} La {{Lettre}}},
  volume = {13},
  isbn = {978-1-84950-974-9 978-1-84950-975-6},
  shorttitle = {Hayek},
  language = {en},
  booktitle = {Advances in {{Austrian Economics}}},
  publisher = {{Emerald Group Publishing Limited}},
  doi = {10.1108/S1529-2134(2010)0000013008},
  author = {Marsh, Leslie},
  editor = {Butos, William N.},
  month = jan,
  year = {2010},
  pages = {115-155},
  file = {/Users/audrey/Zotero/storage/JU4V84FA/Marsh - 2010 - Hayek Cognitive scientist Avant la Lettre.pdf}
}

@article{axtell2016hayek,
  title = {Hayek Enriched by Complexity Enriched by Hayek\ding{73}', Revisiting Hayek's Political Economy (Advances in Austrian Economics, Volume 21)},
  author = {Axtell, Robert L},
  year = {2016},
  publisher = {{Emerald Group Publishing Limited}}
}

@book{georgeBrainComputer1973,
  address = {{Oxford, New York}},
  edition = {2d ed.},
  series = {International Series of Monographs on Pure and Applied Biology. {{Division}}: {{Zoology}}, v. 8},
  title = {The Brain as a Computer},
  isbn = {978-0-08-017022-0},
  lccn = {Q310 .G43 1973},
  publisher = {{Pergamon Press}},
  author = {George, F. H.},
  year = {1973},
  keywords = {Cybernetics},
  file = {/Users/audrey/Zotero/storage/6G3ZB8F8/George - 1973 - The brain as a computer.pdf}
}

@misc{krafftIntroductionFairnessAccountability2019,
  title = {Introduction to {{Fairness}}, {{Accountability}}, and {{Transparency}} in {{Machine Learning}} ({{Reading List}})},
  author = {Krafft, Peaks},
  year = {2019},
  file = {/Users/audrey/Zotero/storage/T9N448W2/OII_MSc_SDS_Introduction to Fairness, Accountabili.pdf}
}

@misc{beardDeepLearningNeoliberal2019,
  title = {Deep {{Learning}} as {{Neoliberal Practice Brainstorming}}},
  author = {Beard, Audrey and Steele, Jamie},
  month = dec,
  year = {2019},
  file = {/Users/audrey/Zotero/storage/UPW2LKZR/paper brainstorm beard & Steele 12.3.19.m4a}
}

@misc{ahmadAlgorithmsHavePolitics2015,
  title = {Do {{Algorithms}} Have {{Politics}}?},
  abstract = {(Heatmap of users tweeting the N word in the US, from the Geography of Hate project at Humbolt University) In Do Artifacts have politics?~Langdon Winner identifies that certain technologies are dem\ldots{}},
  language = {en-US},
  journal = {Muhammad Aurangzeb Ahmad},
  author = {Ahmad, Muhammad Aurangzeb},
  month = may,
  year = {2015},
  file = {/Users/audrey/Zotero/storage/4DS66WW9/do-algorithms-have-politics.html}
}

@misc{maccarthyEthicalCharacterAlgorithms2019,
  title = {The {{Ethical Character}} of {{Algorithms}}\textemdash{}and {{What It Means}} for {{Fairness}}, the {{Character}} of {{Decision}}-{{Making}}, and the {{Future}} of {{News}}},
  abstract = {It is often impossible to choose between competing algorithms without making ethical judgments. They implicate basic notions of fairness, they change the character of decision-making, and they have political implications for the future of news.},
  language = {en-US},
  journal = {The Ethical Machine},
  howpublished = {https://ai.shorensteincenter.org/ideas/2019/1/14/the-ethical-character-of-algorithmsand-what-it-means-for-fairness-the-character-of-decision-making-and-the-future-of-news-yak6m},
  author = {MacCarthy, Mark},
  month = mar,
  year = {2019},
  file = {/Users/audrey/Zotero/storage/27SB4I28/the-ethical-character-of-algorithmsand-what-it-means-for-fairness-the-character-of-decision-mak.html}
}

@misc{zegeyeAlgorithmsHavePolitics2019,
  title = {Do {{Algorithms Have Politics}}? | {{CCTP}}-607: "{{Big Ideas}}": {{AI}} to the {{Cloud}}},
  shorttitle = {Do {{Algorithms Have Politics}}?},
  language = {en-US},
  author = {Zegeye, Adey},
  month = mar,
  year = {2019},
  file = {/Users/audrey/Zotero/storage/4UCQRND3/do-algorithms-have-politics.html}
}

@article{reutersAmazonDitchedAI2018,
  chapter = {Technology},
  title = {Amazon Ditched {{AI}} Recruiting Tool That Favored Men for Technical Jobs},
  issn = {0261-3077},
  abstract = {Specialists had been building computer programs since 2014 to review r{\'e}sum{\'e}s in an effort to automate the search process},
  language = {en-GB},
  journal = {The Guardian},
  author = {Reuters},
  month = oct,
  year = {2018},
  keywords = {Amazon,Artificial intelligence (AI)},
  file = {/Users/audrey/Zotero/storage/IQ475VGQ/amazon-hiring-ai-gender-bias-recruiting-engine.html}
}

@article{dastinAmazonScrapsSecret2018,
  title = {Amazon Scraps Secret {{AI}} Recruiting Tool That Showed Bias against Women},
  abstract = {Amazon.com Inc's machine-learning specialists uncovered a big problem: thei...},
  language = {en},
  journal = {Reuters},
  author = {Dastin, Jeffrey},
  month = oct,
  year = {2018},
  keywords = {All Retail,AMAZON,AUTOMATION,COM,Company News,Department Stores (TRBC),Enterprise Reporting,Europe,General News,Government / Politics,Graphics,Human Rights / Civil Rights,Information Technologies / Computer Sciences,INSIGHT,Insights,Internet / World Wide Web,JOBS,Labour / Personnel,Major News,Pictures,Science,Society / Social Issues,Software and IT Services (TRBC),Special Reports,Technology (TRBC),US,Video,Women's Issues},
  file = {/Users/audrey/Zotero/storage/DQBYIIQZ/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G.html}
}

@article{machlupFriedrichHayekContribution1974,
  title = {Friedrich {{Von Hayek}}'s {{Contribution}} to {{Economics}}},
  volume = {76},
  issn = {00397318},
  number = {4},
  journal = {The Swedish Journal of Economics},
  doi = {10.2307/3439255},
  author = {Machlup, Fritz},
  month = dec,
  year = {1974},
  pages = {498},
  file = {/Users/audrey/Zotero/storage/CEAHJ79T/Machlup_1974_Friedrich Von Hayek's Contribution to Economics.pdf}
}

@book{hayekRoadSerfdomText2007,
  address = {{Chicago}},
  edition = {Definitive ed},
  series = {The Collected Works of {{F}}.{{A}}. {{Hayek}}},
  title = {The Road to Serfdom: Text and Documents},
  isbn = {978-0-226-32054-0 978-0-226-32055-7},
  lccn = {HD82 .H38 2007},
  shorttitle = {The Road to Serfdom},
  number = {v. 2},
  publisher = {{University of Chicago Press}},
  author = {von Hayek, Friedrich A. and Caldwell, Bruce and von Hayek, Friedrich A.},
  year = {2007},
  keywords = {Economic policy,Totalitarianism},
  file = {/Users/audrey/Zotero/storage/MX9ELK6Z/serfdom.pdf},
  note = {OCLC: ocm67383880}
}

@incollection{eubanksAleghenyAlgorithm,
  title = {The {{Alegheny Algorithm}}},
  booktitle = {Automating {{Inequality}}},
  author = {Eubanks, Virginia},
  file = {/Users/audrey/Zotero/storage/ENVFRP8M/Eubanks_The Alegheny Algorithm.pdf}
}

@inproceedings{skirpanDesigningMoralCompass2017,
  address = {{Honolulu, HI, USA}},
  title = {Designing a {{Moral Compass}} for the {{Future}} of {{Computer Vision Using Speculative Analysis}}},
  isbn = {978-1-5386-0733-6},
  abstract = {In this paper we discuss and analyze possible futures for technologies in the field of computer vision (CV). Using a method we have coined speculative analysis we take a broad look at research trends in the field to categorize risks, analyze which ones are most threatening and likely, and ultimately summarize conclusions for how the field may attempt to stem future harms caused by CV technologies. We develop narrative case studies to provoke dialogue and deeply explore possible risk scenarios we found to be most probable and severe. We arrive at the position that there are serious potentials for CV to cause discriminatory harm and exacerbate cybersecurity issues.},
  language = {en},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  publisher = {{IEEE}},
  doi = {10.1109/CVPRW.2017.179},
  author = {Skirpan, Michael and Yeh, Tom},
  month = jul,
  year = {2017},
  pages = {1368-1377},
  file = {/Users/audrey/Zotero/storage/VK8HLBFP/Skirpan and Yeh - 2017 - Designing a Moral Compass for the Future of Comput.pdf}
}

@article{umbrelloFutureWarCould2019,
  title = {The Future of War: Could Lethal Autonomous Weapons Make Conflict More Ethical?},
  issn = {0951-5666, 1435-5655},
  shorttitle = {The Future of War},
  abstract = {Lethal Autonomous Weapons (LAWs) are robotic weapon systems, primarily of value to the military, that could engage in offensive or defensive actions without human intervention. This paper assesses and engages the current arguments for and against the use of LAWs through the lens of achieving more ethical warfare. Specific interest is given particularly to ethical LAWs, which are artificially intelligent weapon systems that make decisions within the bounds of their ethics-based code. To ensure that a wide, but not exhaustive, survey of the implications of employing such ethical devices to replace humans in warfare is taken into account, this paper will engage on matters related to current scholarship on the rejection or acceptance of LAWs\textemdash{}including contemporary technological shortcomings of LAWs to differentiate between targets and the behavioral and psychological volatility of humans\textemdash{}and current and proposed regulatory infrastructures for developing and using such devices. After careful consideration of these factors, this paper will conclude that only ethical LAWs should be used to replace human involvement in war, and, by extension of their consistent abilities, should remove humans from war until a more formidable discovery is made in conducting ethical warfare.},
  language = {en},
  journal = {AI \& SOCIETY},
  doi = {10.1007/s00146-019-00879-x},
  author = {Umbrello, Steven and Torres, Phil and De Bellis, Angelo F.},
  month = feb,
  year = {2019},
  file = {/Users/audrey/Zotero/storage/QL4YCCKH/Umbrello et al. - 2019 - The future of war could lethal autonomous weapons.pdf}
}

@article{saltzIntegratingEthicsMachinelearning2019,
  title = {Integrating {{Ethics}} within {{Machine}}-Learning {{Courses}}},
  volume = {19},
  issn = {19466226},
  language = {en},
  number = {4},
  journal = {ACM Transactions on Computing Education},
  doi = {10.1145/3341164},
  author = {Saltz, Jeffrey and Skirpan, Michael and Fiesler, Casey and Gorelick, Micha and Yeh, Tom and Heckman, Robert and Dewar, Neil and Beard, Nathan},
  month = aug,
  year = {2019},
  pages = {1-26},
  file = {/Users/audrey/Zotero/storage/3EMPFN2U/Saltz et al. - 2019 - Integrating Ethics within Machine-learning Courses.pdf}
}

@article{sloaneINEQUALITYNAMEGAME,
  title = {{{INEQUALITY IS THE NAME OF THE GAME}}. {{THOUGHTS ON THE EMERGING FIELD OF TECHNOLOGY}}, {{ETHICS AND SOCIAL JUSTICE}}.},
  abstract = {This paper argues that the hype around `ethics' as panacea for remedying algorithmic discrimination is a smokescreen for carrying on with business as usual. First, it analyses how the current discourses around digital innovation and algorithmic technologies (including artificial intelligence or AI), newly emerging technology policy and governmental funding patterns as well as global industry developments are currently re-configured around `ethical' considerations. Here, the paper shows how this phenomenon can be broken down into policy approaches and technological approaches. Second, it sets out to provide three pillars for a sociological framework that can help reconceptualize the algorithmic harm and discrimination as an issue of social inequality, rather than ethics. Here, it builds on works on data classification, human agency in design and intersectional inequality. To conclude, the paper suggests three pragmatic steps that should be taken in order to center social justice in technology policy and computer science education.},
  language = {en},
  author = {Sloane, Mona},
  pages = {10},
  file = {/Users/audrey/Zotero/storage/Z8ESXFV5/Sloane - INEQUALITY IS THE NAME OF THE GAME. THOUGHTS ON TH.pdf}
}

@misc{broganGoogleImageRecognition2015,
  title = {Google's Image Recognition Software Returns Some Surprisingly Racist Results.},
  howpublished = {https://slate.com/technology/2015/06/google-s-image-recognition-software-returns-some-surprisingly-racist-results.html},
  author = {Brogan, Jacob},
  month = jun,
  year = {2015},
  file = {/Users/audrey/Zotero/storage/PC8GYYGX/google-s-image-recognition-software-returns-some-surprisingly-racist-results.html}
}

@article{greenDataSciencePolitical,
  title = {Data {{Science}} as {{Political Action}}},
  abstract = {In response to numerous recent controversies, the field of data science has rushed to adopt codes of ethics. Such professional codes, however, are ill-equipped to address broad matters of social justice. Instead of ethics codes, I argue, the field must embrace politics (by which I mean not simply debates about specific political parties and candidates but more broadly the collective social processes that influence rights, status, and resources across society). Data scientists must recognize themselves as political actors engaged in normative constructions of society and, as befits political work, evaluate their work according to its downstream material impacts on people's lives.},
  language = {en},
  author = {Green, Ben},
  pages = {59},
  file = {/Users/audrey/Zotero/storage/SB9LWB5D/Green - Data Science as Political Action.pdf}
}

@article{abebeRolesComputingSocial2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1912.04883},
  primaryClass = {cs},
  title = {Roles for {{Computing}} in {{Social Change}}},
  abstract = {A recent normative turn in computer science has brought concerns about fairness, bias, and accountability to the core of the field. Yet recent scholarship has warned that much of this technical work treats problematic features of the status quo as fixed, and fails to address deeper patterns of injustice and inequality. While acknowledging these critiques, we posit that computational research has valuable roles to play in addressing social problems \textemdash{} roles whose value can be recognized even from a perspective that aspires toward fundamental social change. In this paper, we articulate four such roles, through an analysis that considers the opportunities as well as the significant risks inherent in such work. Computing research can serve as a diagnostic, helping us to understand and measure social problems with precision and clarity. As a formalizer, computing shapes how social problems are explicitly defined \textemdash{} changing how those problems, and possible responses to them, are understood. Computing serves as rebuttal when it illuminates the boundaries of what is possible through technical means. And computing acts as synecdoche when it makes long-standing social problems newly salient in the public eye. We offer these paths forward as modalities that leverage the particular strengths of computational work in the service of social change, without overclaiming computing's capacity to solve social problems on its own.},
  language = {en},
  journal = {arXiv:1912.04883 [cs]},
  doi = {10.1145/3351095.3372871},
  author = {Abebe, Rediet and Barocas, Solon and Kleinberg, Jon and Levy, Karen and Raghavan, Manish and Robinson, David G.},
  month = dec,
  year = {2019},
  keywords = {Computer Science - Computers and Society},
  file = {/Users/audrey/Zotero/storage/GMFW7KGG/Abebe et al_2019_Roles for Computing in Social Change.pdf}
}

@incollection{demilioCapitalismGayIdentity1993,
  address = {{Great Britain}},
  title = {Capitalism and {{Gay Identity}}},
  booktitle = {The {{Lesbian}} and {{Gay Studies Reader}}},
  publisher = {{Routledge}},
  author = {D'Emilio, John},
  editor = {Abelove, Henry and Barale, Mich{\`e}le Aina and Halperin, David M.},
  year = {1993},
  file = {/Users/audrey/Zotero/storage/AQAUCU9X/D'Emilio_1993_Capitalism and Gay Identity.pdf}
}

@inproceedings{greenGoodIsnGood2019,
  title = {"{{Good}}" Isn't Good Enough},
  booktitle = {{{AI}} for {{Social Good}}},
  publisher = {{ACM}},
  author = {Green, Ben},
  year = {2019},
  keywords = {_tablet},
  file = {/Users/audrey/Documents/Zotfiles/Green_2019_Good isn't good enough.pdf}
}

@article{kantorAmazonWrestlingBig2015,
  title = {Inside {{Amazon}}: {{Wrestling Big Ideas}} in a {{Bruising Workplace}} - {{The New York Times}}},
  journal = {New York Times},
  author = {Kantor, Jodi and Streitfeld, David},
  month = aug,
  year = {2015},
  file = {/Users/audrey/Zotero/storage/MMZSY8MB/inside-amazon-wrestling-big-ideas-in-a-bruising-workplace.html}
}

@article{boshoffNeocolonialismResearchCollaboration2009,
  title = {Neo-Colonialism and Research Collaboration in {{Central Africa}}},
  volume = {81},
  issn = {0138-9130, 1588-2861},
  language = {en},
  number = {2},
  journal = {Scientometrics},
  doi = {10.1007/s11192-008-2211-8},
  author = {Boshoff, Nelius},
  month = nov,
  year = {2009},
  keywords = {_tablet},
  pages = {413-434},
  file = {/Users/audrey/Zotero/storage/CTEZLGPE/Boshoff_2009_Neo-colonialism and research collaboration in Central Africa.pdf}
}

@article{bellerInformaticLaborAge2016,
  title = {Informatic {{Labor}} in the {{Age}} of {{Computational Capital}}},
  volume = {5},
  issn = {24694053},
  abstract = {Jonathan Beller expands conversations about the role of the digital and the digital humanities through attention to the mechanisms by which the digital image is instrumental in neoliberal capitalist accumulation and colonialism. Beller argues that the digital image itself exploits the attentive labor of those who see it, organizes profitable patterns of spectatorship, and links communication directly to financial speculation. Through scrutiny of examples that attempt to disrupt the profitable, algorithmically-capitalized flow of data and attention through the interface of the screen, Beller's article makes a pointed critique of the ways that fascism manifests in and might be combated via digital economies.},
  language = {en},
  number = {1},
  journal = {Lateral},
  doi = {10.25158/L5.1.4},
  author = {Beller, Jonathan},
  month = may,
  year = {2016},
  keywords = {_tablet},
  file = {/Users/audrey/Zotero/storage/35A9B5AS/Beller_2016_Informatic Labor in the Age of Computational Capital.pdf}
}

@article{pourisResearchEmphasisCollaboration2014,
  title = {Research Emphasis and Collaboration in {{Africa}}},
  volume = {98},
  issn = {0138-9130, 1588-2861},
  abstract = {Scientific co-authorship of African researchers has become a fashionable topic in the recent scientometric literature. Researchers are investigating the effects, modes, dynamics and motives of collaboration in a continental research system which is in an embryonic stage and in different stages of development from country to country. In this article we attempt to provide some additional evidence by examining both patterns of collaboration at country and continental levels and the scientific disciplines emphasised. Our findings indicate that the continent's research emphasises medical and natural resources disciplines to the detriment of disciplines supporting knowledge based economies and societies. Furthermore, we identify that the collaborative patterns in Africa are substantial higher than in the rest of the world. A number of questions related to research collaboration and its effects are raised.},
  language = {en},
  number = {3},
  journal = {Scientometrics},
  doi = {10.1007/s11192-013-1156-8},
  author = {Pouris, Anastassios and Ho, Yuh-Shan},
  month = mar,
  year = {2014},
  keywords = {_tablet},
  pages = {2169-2184},
  file = {/Users/audrey/Documents/Zotfiles/Pouris_Ho_2014_Research emphasis and collaboration in Africa.pdf}
}

@article{parkerGoodBadResearch2016,
  title = {Good and {{Bad Research Collaborations}}: {{Researchers}}' {{Views}} on {{Science}} and {{Ethics}} in {{Global Health Research}}},
  volume = {11},
  issn = {1932-6203},
  shorttitle = {Good and {{Bad Research Collaborations}}},
  language = {en},
  number = {10},
  journal = {PLOS ONE},
  doi = {10.1371/journal.pone.0163579},
  author = {Parker, Michael and Kingori, Patricia},
  editor = {Fortin, Anny},
  month = oct,
  year = {2016},
  keywords = {_tablet},
  pages = {e0163579},
  file = {/Users/audrey/Documents/Zotfiles/Parker_Kingori_2016_Good and Bad Research Collaborations.pdf}
}

@article{ponomariovKnowledgeFlowsBases2014,
  title = {Knowledge Flows and Bases in Emerging Economy Innovation Systems: {{Brazilian}} Research 2005\textendash{}2009},
  volume = {43},
  issn = {00487333},
  shorttitle = {Knowledge Flows and Bases in Emerging Economy Innovation Systems},
  abstract = {This article considers the role of domestic knowledge capabilities for developing countries and emerging economies, and in particular in the build-up of their national systems of innovation. Using bibliometric methods, we describe the geographic sources of knowledge and the users of Brazilian research in 2005\textendash{}2009, and analyze the roles of domestic and foreign knowledge bases in it. Our results suggest that increasing reliance on domestic sources of knowledge is a feature of Brazil's improved science and technology capabilities. The ascendancy of Brazil's research informs us about the unfolding re-organization of global research, too, underlining nascent South-South knowledge flows, the prevailing relevance of EU research, and the relative decline of US research for Brazilian knowledge creation.},
  language = {en},
  number = {3},
  journal = {Research Policy},
  doi = {10.1016/j.respol.2013.09.002},
  author = {Ponomariov, Branco and Toivanen, Hannes},
  month = apr,
  year = {2014},
  keywords = {_tablet},
  pages = {588-596},
  file = {/Users/audrey/Documents/Zotfiles/Ponomariov_Toivanen_2014_Knowledge flows and bases in emerging economy innovation systems.pdf}
}

@article{mitchellModelCardsModel2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1810.03993},
  title = {Model {{Cards}} for {{Model Reporting}}},
  abstract = {Trained machine learning models are increasingly used to perform high-impact tasks in areas such as law enforcement, medicine, education, and employment. In order to clarify the intended use cases of machine learning models and minimize their usage in contexts for which they are not well suited, we recommend that released models be accompanied by documentation detailing their performance characteristics. In this paper, we propose a framework that we call model cards, to encourage such transparent model reporting. Model cards are short documents accompanying trained machine learning models that provide benchmarked evaluation in a variety of conditions, such as across different cultural, demographic, or phenotypic groups (e.g., race, geographic location, sex, Fitzpatrick skin type [15]) and intersectional groups (e.g., age and race, or sex and Fitzpatrick skin type) that are relevant to the intended application domains. Model cards also disclose the context in which models are intended to be used, details of the performance evaluation procedures, and other relevant information. While we focus primarily on human-centered machine learning models in the application fields of computer vision and natural language processing, this framework can be used to document any trained machine learning model. To solidify the concept, we provide cards for two supervised models: One trained to detect smiling faces in images, and one trained to detect toxic comments in text. We propose model cards as a step towards the responsible democratization of machine learning and related artificial intelligence technology, increasing transparency into how well artificial intelligence technology works. We hope this work encourages those releasing trained machine learning models to accompany model releases with similar detailed evaluation numbers and other relevant documentation.},
  language = {en},
  journal = {Proceedings of the Conference on Fairness, Accountability, and Transparency - FAT* '19},
  doi = {10.1145/3287560.3287596},
  author = {Mitchell, Margaret and Wu, Simone and Zaldivar, Andrew and Barnes, Parker and Vasserman, Lucy and Hutchinson, Ben and Spitzer, Elena and Raji, Inioluwa Deborah and Gebru, Timnit},
  year = {2019},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  pages = {220-229},
  file = {/Users/audrey/Zotero/storage/QR78WF95/Mitchell et al. - 2019 - Model Cards for Model Reporting.pdf}
}

@article{korac-kakabadseInformationTechnologyDevelopment2000,
  title = {Information Technology and Development: Creating `{{IT}} Harems', Fostering New Colonialism or Solving `Wicked' Policy Problems?},
  volume = {20},
  issn = {1099-162X},
  shorttitle = {Information Technology and Development},
  abstract = {The proliferation of information technology offers challenges to developing countries which struggle with basic human needs. Yet the key to their survival may lie in information which is inaccessible to them. Facing these challenges, developing nations start from a position of frailty based on low levels of capital; a limited information infrastructure; dependencies on foreign aid and multinationals; and an ever-increasing population growth. It is essential that foreign technology inflow is adopted strategically within the pre-existing framework of national policies for technological development and with an emphasis on technology transfer. The broad policy direction needs to be towards the establishment of an information infrastructure and a contingent perspective for the meta-policy process of designing appropriate information technology infrastructures. Copyright \textcopyright{} 2000 John Wiley \& Sons, Ltd.},
  language = {en},
  number = {3},
  journal = {Public Administration and Development},
  doi = {10.1002/1099-162X(200008)20:3<171::AID-PAD141>3.0.CO;2-9},
  author = {Korac-Kakabadse, Nada and Kouzmin, Alexander and Korac-Kakabadse, Andrew},
  year = {2000},
  pages = {171-184},
  file = {/Users/audrey/Documents/Zotfiles/Korac‐Kakabadse et al_2000_Information technology and development.pdf;/Users/audrey/Zotero/storage/SYF3984U/1099-162X(200008)203171AID-PAD1413.0.html}
}

@inproceedings{parhamAnimalDetectionPipeline2018,
  address = {{Lake Tahoe, NV}},
  title = {An {{Animal Detection Pipeline}} for {{Identification}}},
  isbn = {978-1-5386-4886-5},
  abstract = {This paper proposes a 5-component detection pipeline for use in a computer vision-based animal recognition system. The end result of our proposed pipeline is a collection of novel annotations of interest (AoI) with species and viewpoint labels. These AoIs, for example, could be fed as the focused input data into an appearance-based animal identification system. The goal of our method is to increase the reliability and automation of animal censusing studies and to provide better ecological information to conservationists. Our method is able to achieve a localization mAP of 81.67\%, a species and viewpoint annotation classification accuracy of 94.28\% and 87.11\%, respectively, and an AoI accuracy of 72.75\% across 6 animal species of interest. We also introduce the Wildlife Image and Localization Dataset (WILD), which contains 5,784 images and 12,007 labeled annotations across 28 classification species and a variety of challenging, real-world detection scenarios.},
  language = {en},
  booktitle = {2018 {{IEEE Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  publisher = {{IEEE}},
  doi = {10.1109/WACV.2018.00123},
  author = {Parham, Jason and Stewart, Charles and Crall, Jonathan and Rubenstein, Daniel and Holmberg, Jason and {Berger-Wolf}, Tanya},
  month = mar,
  year = {2018},
  pages = {1075-1083},
  file = {/Users/audrey/Zotero/storage/8EP8NRYX/Parham et al. - 2018 - An Animal Detection Pipeline for Identification.pdf}
}

@article{berger-wolfWildbookCrowdsourcingComputer2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1710.08880},
  primaryClass = {cs},
  title = {Wildbook: {{Crowdsourcing}}, Computer Vision, and Data Science for Conservation},
  shorttitle = {Wildbook},
  abstract = {Photographs, taken by field scientists, tourists, automated cameras, and incidental photographers, are the most abundant source of data on wildlife today. Wildbook is an autonomous computational system that starts from massive collections of images and, by detecting various species of animals and identifying individuals, combined with sophisticated data management, turns them into high resolution information database, enabling scientific inquiry, conservation, and citizen science. We have built Wildbooks for whales (flukebook.org), sharks (whaleshark.org), two species of zebras (Grevy's and plains), and several others. In January 2016, Wildbook enabled the first ever full species (the endangered Grevy's zebra) census using photographs taken by ordinary citizens in Kenya. The resulting numbers are now the official species census used by IUCN Red List: http://www.iucnredlist.org/details/7950/0. In 2016, Wildbook partnered up with WWF to build Wildbook for Sea Turtles, Internet of Turtles (IoT), as well as systems for seals and lynx. Most recently, we have demonstrated that we can now use publicly available social media images to count and track wild animals. In this paper we present and discuss both the impact and challenges that the use of crowdsourced images can have on wildlife conservation.},
  language = {en},
  journal = {arXiv:1710.08880 [cs]},
  author = {{Berger-Wolf}, Tanya Y. and Rubenstein, Daniel I. and Stewart, Charles V. and Holmberg, Jason A. and Parham, Jason and Menon, Sreejith and Crall, Jonathan and Van Oast, Jon and Kiciman, Emre and Joppa, Lucas},
  month = oct,
  year = {2017},
  keywords = {Computer Science - Computers and Society},
  file = {/Users/audrey/Zotero/storage/WBIDUNKX/Berger-Wolf et al. - 2017 - Wildbook Crowdsourcing, computer vision, and data.pdf}
}

@misc{YoshuaBengioFacebook,
  title = {Yoshua {{Bengio}}'s {{Facebook Page}}},
  howpublished = {https://www.facebook.com/yoshua.bengio},
  file = {/Users/audrey/Zotero/storage/TPE79CEH/yoshua.html}
}

@misc{bengioYoshuaBengioPressed2019,
  type = {Social {{Media}}},
  title = {Yoshua {{Bengio}} - {{Pressed}} by Several, Including {{Gary Marcus}}, To...},
  journal = {facebook.com},
  howpublished = {https://www.facebook.com/yoshua.bengio/posts/2269432439828350},
  author = {Bengio, Yoshua},
  month = dec,
  year = {2019},
  keywords = {\#aidebate,ai,ai debate,deep learning,definition,facebook,social media},
  file = {/Users/audrey/Zotero/storage/TQ8WD4B4/2269432439828350.html}
}

@misc{lecunYannLeCunFolks2019,
  type = {Social {{Media}}},
  title = {Yann {{LeCun}} - {{Some}} Folks Still Seem Confused about What Deep...},
  journal = {facebook.com},
  howpublished = {https://www.facebook.com/yann.lecun/posts/10156463919392143},
  author = {LeCun, Yann},
  month = dec,
  year = {2019},
  keywords = {\#aidebate,ai,ai debate,deep learning,definition,facebook,social media},
  file = {/Users/audrey/Zotero/storage/KG6N9ZVK/10156463919392143.html}
}

@misc{lecunYannLeCunOK2018,
  type = {Social {{Media}}},
  title = {Yann {{LeCun}} - {{OK}}, {{Deep Learning}} Has Outlived Its Usefulness as a...},
  journal = {facebook.com},
  howpublished = {https://www.facebook.com/yann.lecun/posts/10155003011462143},
  author = {LeCun, Yann},
  month = jan,
  year = {2018},
  keywords = {\#aidebate,ai,ai debate,deep learning,definition,facebook,social media},
  file = {/Users/audrey/Zotero/storage/XN38KD3V/10155003011462143.html}
}

@inproceedings{lecunGradientBasedLearningApplied1998,
  title = {Gradient-{{Based Learning Applied}} to {{Document Recognition}}},
  publisher = {{IEEE}},
  author = {LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  month = nov,
  year = {1998},
  file = {/Users/audrey/Zotero/storage/VYRAVHR7/lecun-01a.pdf}
}

@article{joLessonsArchivesStrategies2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1912.10389},
  primaryClass = {cs},
  title = {Lessons from {{Archives}}: {{Strategies}} for {{Collecting Sociocultural Data}} in {{Machine Learning}}},
  shorttitle = {Lessons from {{Archives}}},
  abstract = {A growing body of work shows that many problems in fairness, accountability, transparency, and ethics in machine learning systems are rooted in decisions surrounding the data collection and annotation process. In spite of its fundamental nature however, data collection remains an overlooked part of the machine learning (ML) pipeline. In this paper, we argue that a new specialization should be formed within ML that is focused on methodologies for data collection and annotation: efforts that require institutional frameworks and procedures. Specifically for sociocultural data, parallels can be drawn from archives and libraries. Archives are the longest standing communal effort to gather human information and archive scholars have already developed the language and procedures to address and discuss many challenges pertaining to data collection such as consent, power, inclusivity, transparency, and ethics \& privacy. We discuss these five key approaches in document collection practices in archives that can inform data collection in sociocultural ML. By showing data collection practices from another field, we encourage ML research to be more cognizant and systematic in data collection and draw from interdisciplinary expertise.},
  language = {en},
  journal = {arXiv:1912.10389 [cs]},
  doi = {10.1145/3351095.3372829},
  author = {Jo, Eun Seo and Gebru, Timnit},
  month = dec,
  year = {2019},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society,Computer Science - Machine Learning,I.2.0},
  file = {/Users/audrey/Zotero/storage/HEIBK65N/Jo and Gebru - 2019 - Lessons from Archives Strategies for Collecting S.pdf}
}


